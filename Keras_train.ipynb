{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villasen/small-urban-sound-dataset/blob/master/Keras_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XfXFsAnRkn9",
        "colab_type": "text"
      },
      "source": [
        "#Urban dataset download and file structure setup for train, validation, and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6mpdiRj4-Fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/villasen/small-urban-sound-dataset.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qrPrM21WXMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r /content/small-urban-sound-dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD6xOVD6ho2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r /content/target_npy_files/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQz67Mm9GIUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from preprocess import *\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras import utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow\n",
        "import scipy\n",
        "import os, shutil\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target_dir = '/content/target_npy_files'\n",
        "DATA_PATH = \"small-urban-sound-dataset/tiny-dataset/\"\n",
        "os.mkdir(target_dir)\n",
        "\n",
        "def wav2mfcc(file_path, max_pad_len=51):\n",
        "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
        "    mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=10, n_fft=640, hop_length=320)\n",
        "    pad_width = max_pad_len - mfcc.shape[1]\n",
        "    if pad_width < 0: \n",
        "      print(mfcc.shape[1])\n",
        "      print(pad_width)\n",
        "      print(\"error in \"+ file_path)\n",
        "    \n",
        "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    return mfcc\n",
        "\n",
        "  \n",
        "  \n",
        "def get_labels(path=DATA_PATH):\n",
        "    labels = os.listdir(path) \n",
        "    label_indices = np.arange(0, len(labels))\n",
        "    return labels, label_indices, to_categorical(label_indices)  \n",
        "\n",
        "  \n",
        "def save_data_to_array(path=DATA_PATH, max_pad_len=11):\n",
        "    labels, _, _ = get_labels(path)\n",
        "\n",
        "    for label in labels:\n",
        "        # Init mfcc vectors\n",
        "        mfcc_vectors = []\n",
        "\n",
        "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
        "        for wavfile in wavfiles:\n",
        "            #print(path + label + '/' + wavfile)\n",
        "            #print(label)\n",
        "            if label == '_background_noise_' : break\n",
        "            mfcc = wav2mfcc(wavfile, max_pad_len=max_pad_len)\n",
        "            \n",
        "            mfcc_vectors.append(mfcc)\n",
        "        np.save('/content/target_npy_files/' + label + '.npy', mfcc_vectors)\n",
        "        print(label)\n",
        "        print(len(mfcc_vectors))\n",
        "\n",
        "        \n",
        "        \n",
        "def save_data_to_array(path=DATA_PATH, max_pad_len=11):\n",
        "    labels, _, _ = get_labels(path)\n",
        "\n",
        "    for label in labels:\n",
        "        # Init mfcc vectors\n",
        "        mfcc_vectors = []\n",
        "\n",
        "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
        "        for wavfile in wavfiles:\n",
        "            #print(path + label + '/' + wavfile)\n",
        "            #print(label)\n",
        "            if label == '_background_noise_' : break\n",
        "            mfcc = wav2mfcc(wavfile, max_pad_len=max_pad_len)\n",
        "            \n",
        "            mfcc_vectors.append(mfcc)\n",
        "        np.save('/content/target_npy_files/' + label + '.npy', mfcc_vectors)\n",
        "        print(label)\n",
        "        print(len(mfcc_vectors))\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "def get_train_test(split_ratio=0.9, random_state=42):\n",
        "    # Get available labels\n",
        "    labels, indices, _ = get_labels(DATA_PATH)\n",
        "\n",
        "    # Getting first arrays\n",
        "    X = np.load('/content/target_npy_files/' + labels[0] + '.npy')\n",
        "    y = np.zeros(X.shape[0])\n",
        "\n",
        "    #X.ndim\n",
        "    # Append all of the dataset into one single array, same goes for y\n",
        "    for i, label in enumerate(labels[1:]):\n",
        "        x = np.load('/content/target_npy_files/' + label + '.npy')\n",
        "        \n",
        "       # X.ndim\n",
        "       # x.ndim\n",
        "        X = np.vstack((X, x))\n",
        "        y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
        "        \n",
        "    assert X.shape[0] == len(y)\n",
        "\n",
        "    return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "save_data_to_array(path=DATA_PATH, max_pad_len=51)    \n",
        "X_train, X_test, y_train, y_test = get_train_test()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 10, 51, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 10, 51, 1)\n",
        "y_train_hot = to_categorical(y_train)\n",
        "y_test_hot = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvG5N6W6MRLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r /content/target_npy_files/*.npy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt84MsB5FYlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = get_train_test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiImgcPYHci2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eshFoHU1IYCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 10, 51, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 10, 51, 1)\n",
        "y_train_hot = to_categorical(y_train)\n",
        "y_test_hot = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWZNjHYTtLnS",
        "colab_type": "text"
      },
      "source": [
        "# Building DS-CNN model using Keras framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWsjPl-K0IE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating Keras sequential model\n",
        "#bn = 1\n",
        "BN=False\n",
        "model = models.Sequential()\n",
        "\n",
        "def dscnn_train():\n",
        "  # 1\n",
        "      model.add(layers.Conv2D(64, (4,10), strides=(2,2), padding='same', activation='relu', \\\n",
        "                #input_shape=(10, 49, 1)))\n",
        "                input_shape=(10,51,1)))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                                center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                                gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                                moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                                gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      # 2\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))  \n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      # 3\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "\n",
        "      # 4\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "\n",
        "      # 5\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:      \n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      # Final layer\n",
        "      model.add(layers.AveragePooling2D(pool_size=(5, 25), strides=(2,2), padding='valid', data_format=None))\n",
        "      model.add(layers.Flatten(data_format=None))\n",
        "      model.add(layers.Dense(64, activation='relu'))\n",
        "      model.add(layers.Dense(12, activation='softmax'))\n",
        "  \n",
        "      # Compilation step to choose loss function, optimizer and metric\n",
        "      # Configuring the learning process\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      #model.fit()\n",
        "      model.fit(X_train, y_train_hot, batch_size=100, epochs=200, verbose=1, validation_data=(X_test, y_test_hot))\n",
        "      \n",
        "      # Restarts layer sequence number \n",
        "      K.clear_session()\n",
        "\n",
        "  \n",
        "  \n",
        "dscnn_train()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQYm890ntf12",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm5FlvfX88wo",
        "colab_type": "text"
      },
      "source": [
        "##Alternate folder train and test structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9njsm5MskSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = wav2mfcc('/content/small-urban-sound-dataset/tiny-dataset/crackling_fire/1-17150-A.wav')\n",
        "\n",
        "#X_train = X_train.reshape(X_train.shape[0], 10, 51, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARF4iPLZXH7t",
        "colab_type": "code",
        "outputId": "e0564145-94e2-4493-ff27-5aed3f3c9168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sample.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7phdcHrqXCat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_reshape = sample.reshape(sample(10,51,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q5xmQdSWFBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(get_labels() [0] [np.argmax(model.predict(sample_reshape))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiLUN--y0u79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHBULmNeFteL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "base_directory ='/content/target_dataset/'\n",
        "os.mkdir(base_directory)\n",
        "train_dir = os.path.join(base_directory, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_directory, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_directory, 'test')\n",
        "os.mkdir(test_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDMMKPgkXSko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_dataset_dir = '/content/Sound-Datasets/combined_datasets/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcURx0euURiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gunshot_dir = os.path.join(train_dir, 'gun_shot')\n",
        "os.mkdir(train_gunshot_dir)\n",
        "train_dogbark_dir = os.path.join(train_dir, 'dog_bark')\n",
        "os.mkdir(train_dogbark_dir)\n",
        "\n",
        "validation_gunshot_dir = os.path.join(validation_dir, 'gun_shot')\n",
        "os.mkdir(validation_gunshot_dir)\n",
        "validation_dogbark_dir = os.path.join(validation_dir, 'dog_bark')\n",
        "os.mkdir(validation_dogbark_dir)\n",
        "\n",
        "test_gunshot_dir = os.path.join(test_dir, 'gun_shot')\n",
        "os.mkdir(test_gunshot_dir)\n",
        "test_dogbark_dir = os.path.join(test_dir, 'dog_bark')\n",
        "os.mkdir(test_dogbark_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ-fuVCQZWWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fnames_dog = os.listdir('/content/Sound-Datasets/combined_datasets/dog_bark')\n",
        "fnames_gun = os.listdir('/content/Sound-Datasets/combined_datasets/gun_shot')\n",
        "fnames = os.listdir('/content/Sound-Datasets/combined_datasets/dog_bark')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54t8veJLZdw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(fnames)\n",
        "size = len(fnames)\n",
        "train_percentage = 0.8\n",
        "validation_percentage = 0.1\n",
        "test_percentage = 0.1\n",
        "print('train percentage = %f' %train_percentage)\n",
        "train_size = int(round(size * (train_percentage)))\n",
        "validation_size = int(round(size * (validation_percentage)))\n",
        "test_size = int(round(size * (test_percentage)))\n",
        "print(size)\n",
        "print('train_size = %i' %train_size)\n",
        "print('validation_size = %i' %validation_size)\n",
        "print('test_size = %i' %test_size)\n",
        "total = train_size + validation_size + test_size\n",
        "print(total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pojp7QvbfKch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#os.chdir('/content/combined_datasets/dog_bark')\n",
        "train_list=[]\n",
        "validation_list=[]\n",
        "test_list=[]\n",
        "\n",
        "train_list = fnames[0:train_size]\n",
        "validation_list = fnames[train_size:train_size+validation_size]\n",
        "test_list = fnames[train_size+validation_size:total]\n",
        "\n",
        "\n",
        "for train in train_list:\n",
        "  src_train = os.path.join(original_dataset_dir+'dog_bark', train )\n",
        "  dst_train = os.path.join('/content/target_dataset/train/dog_bark', train)\n",
        "  shutil.copyfile(src_train, dst_train)\n",
        "\n",
        "for validation in validation_list:\n",
        "  src_validation = os.path.join(original_dataset_dir+'dog_bark', validation)\n",
        "  dst_validation = os.path.join('/content/target_dataset/validation/dog_bark', validation)\n",
        "  shutil.copyfile(src_validation, dst_validation)\n",
        "  \n",
        "for test in test_list:\n",
        "  src_test = os.path.join(original_dataset_dir+'dog_bark', test)\n",
        "  dst_test = os.path.join('/content/target_dataset/test/dog_bark', test)\n",
        "  shutil.copyfile(src_test, dst_test)\n",
        "  \n",
        "        \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOB0kg2_DGCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fnames_train = os.listdir('/content/target_dataset/train/dog_bark')\n",
        "print(len(fnames_train))\n",
        "fnames_val = os.listdir('/content/target_dataset/validation/dog_bark')\n",
        "print(len(fnames_val))\n",
        "fnames_test = os.listdir('/content/target_dataset/test/dog_bark')\n",
        "print(len(fnames_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z94KvPBWU6kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_gunshot_dir = os.path.join(validation_dir, 'gun_shot')\n",
        "os.mkdir(validation_gunshot_dir)\n",
        "validation_dogbark_dir = os.path.join(validation_dir, 'dog_bark')\n",
        "os.mkdir(validation_dogbark_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imuKEMG8U8Jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_gunshot_dir = os.path.join(test_dir, 'gun_shot')\n",
        "os.mkdir(test_gunshot_dir)\n",
        "test_dogbark_dir = os.path.join(test_dir, 'dog_bark')\n",
        "os.mkdir(test_dogbark_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywdXdFQFRUXu",
        "colab_type": "text"
      },
      "source": [
        "###Preparing for google speech dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdio_TpuAE5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_url='http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "os.makedirs(dest_directory)\n",
        "filename = data_url.split('/')[-1]\n",
        "filepath = os.path.join(dest_directory, filename)\n",
        "\n",
        "print(filename)\n",
        "print(filepath)\n",
        " \n",
        "def _progress(count, block_size, total_size):\n",
        "    sys.stdout.write('\\r>> Downloading %s %.1f%%' % \\\n",
        "            (filename, float(count * block_size) / float(total_size) * 100.0)) \n",
        "    sys.stdout.flush()\n",
        "\n",
        "urban_dataset_dir = '/content/combined_datasets/' \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z1f7OCcESAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath, _ = urllib.request.urlretrieve(data_url, filepath, _progress)\n",
        "statinfo = os.stat(filepath)\n",
        "tf.logging.info('Successfully downloaded %s (%d bytes)', filename, statinfo.st_size)\n",
        "tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}